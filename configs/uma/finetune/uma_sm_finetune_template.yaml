defaults:
  - data: ??
  - _self_

job:
  device_type: CUDA
  scheduler:
    mode: LOCAL
    ranks_per_node: 1
    num_nodes: 1
  debug: True
  run_dir: /tmp/uma_finetune_runs/
  run_name: uma_finetune
  logger:
    _target_: fairchem.core.common.logger.WandBSingletonLogger.init_wandb
    _partial_: true
    entity: example
    project: uma_finetune

# refers to a model name that can be retrieved from (huggingface)[https://huggingface.co/facebook/UMA]
base_model_name: uma-s-1p1
# 300 is effectively infinite neighbors, but if you are memory strained, using 100 is usually just as good
max_neighbors: 300
# choose to either run for integer number of epochs or steps, only 1 can be step, the other must be null
epochs: 1
steps: null
# customize batch size depending on your system
batch_size: 2
lr: 4e-4
weight_decay: 1e-3
evaluate_every_n_steps: 100
checkpoint_every_n_steps: 1000


train_dataset:
  _target_: fairchem.core.datasets.mt_concat_dataset.create_concat_dataset
  dataset_configs:
    DATASET_NAME: ${data.train_dataset}
  combined_dataset_config: { sampling: {type: temperature, temperature: 1.0} }

train_dataloader:
  _target_: fairchem.core.components.common.dataloader_builder.get_dataloader
  dataset: ${train_dataset}
  batch_sampler_fn:
    _target_: fairchem.core.common.data_parallel.BalancedBatchSampler
    _partial_: True
    batch_size: ${batch_size}
    shuffle: True
    seed: 0
  num_workers: 0
  collate_fn:
    _target_: fairchem.core.units.mlip_unit.mlip_unit.mt_collater_adapter
    tasks: ${data.tasks_list}

val_dataset:
  _target_: fairchem.core.datasets.mt_concat_dataset.create_concat_dataset
  dataset_configs:
    DATASET_NAME: ${data.val_dataset}
  combined_dataset_config: { sampling: {type: temperature, temperature: 1.0} }

eval_dataloader:
  _target_: fairchem.core.components.common.dataloader_builder.get_dataloader
  dataset: ${val_dataset}
  batch_sampler_fn:
    _target_: fairchem.core.common.data_parallel.BalancedBatchSampler
    _partial_: True
    batch_size: ${batch_size}
    shuffle: False
    seed: 0
  num_workers: 0
  collate_fn:
    _target_: fairchem.core.units.mlip_unit.mlip_unit.mt_collater_adapter
    tasks: ${data.tasks_list}

runner:
  _target_: fairchem.core.components.train.train_runner.TrainEvalRunner
  train_dataloader: ${train_dataloader}
  eval_dataloader: ${eval_dataloader}
  train_eval_unit:
    _target_: fairchem.core.units.mlip_unit.mlip_unit.MLIPTrainEvalUnit
    job_config: ${job}
    tasks: ${data.tasks_list}
    model:
      _target_: fairchem.core.units.mlip_unit.mlip_unit.initialize_finetuning_model
      checkpoint_location:
        _target_: fairchem.core.calculate.pretrained_mlip.pretrained_checkpoint_path_from_name
        model_name: ${base_model_name}
      overrides:
        backbone:
          otf_graph: True
          max_neighbors: ${max_neighbors}
          regress_stress: ${data.regress_stress}
          always_use_pbc: False
        pass_through_head_outputs: ${data.pass_through_head_outputs}
      heads: ${data.heads}
    optimizer_fn:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: ${lr}
      weight_decay: ${weight_decay}
    cosine_lr_scheduler_fn:
      _target_: fairchem.core.units.mlip_unit.mlip_unit._get_consine_lr_scheduler
      _partial_: true
      warmup_factor: 0.2
      warmup_epochs: 0.01
      lr_min_factor: 0.01
      epochs: ${epochs}
      steps: ${steps}
    print_every: 10
    clip_grad_norm: 100
  max_epochs: ${epochs}
  max_steps: ${steps}
  evaluate_every_n_steps: ${evaluate_every_n_steps}
  callbacks:
    - _target_: fairchem.core.components.train.train_runner.TrainCheckpointCallback
      checkpoint_every_n_steps: ${checkpoint_every_n_steps}
      max_saved_checkpoints: 5
    - _target_: torchtnt.framework.callbacks.TQDMProgressBar
